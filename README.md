# przetwarzanie dużych zbiorów PySparku 

- tworzymy kluster z dockerów
- tworzymy foldery ze zbiorami
- przetwarzamy go w PySparku
- maksymalne wykorzystanie elementów funkcyjnych pythona jak:
  - funkcje first class value
  - partial funcions
  - składanie funkcji, lambdy


Implementacja algorytmów minimalnych w sparku i scali 2:

- MapReduce
- Terasort
- Sliding Aggregation
- Perfect Balance Sort
- Prefix Sum
- Ranking
